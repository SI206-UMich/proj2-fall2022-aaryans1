1. 

Throughout this project, we acted as investigators to uphold the system of
accountability created by the San Francisco lawmakers: listers must register with the city’s planning office and put the business license’s number on Airbnb’s website, Airbnb must display some effort in validating these policy numbers, and third parties can register a complaint of illegal short-term rentals with the city planning office. We used web-scraping to do the latter using several hours of our personal time.
Imagine you’re a software developer at either the San Francisco Planning Office (SFPO) or Airbnb.com. Describe a different system that verifies that the business license is valid for short term rentals in San Francisco and list at least two arguments you might hear at your organization (either SFPO or Airbnb.com) against adopting your system.

A different system that verifies that the business license is valid for short term rentals is by cross referencing the policy number with the city planning office's database for valid policies at property rental signup. AirBnB SWE can introduce this validation at signup in order to weed out illegal properties. One argument against this system is the fact that AirBnB would have to gain access to city planning office data which would involve a lot of bureaucracy and time committment. Another argument against this is that AirBnB should not be responsible for valid properties but rather the SFPO should be more vigilant in verification. 

2. The database we’ve created through web-scraping is a great data source of information for data scientists in order to answer and explore research questions. Skim through the Housing Insecurity in the US Wikipedia page and describe at least one research question that you could answer or explore using this data if you were a data scientist working with a housing activist organization to fight against housing insecurity.

One research question you can explore using this data if you were a data scientist is how affordable is housing in different areas based on these rental rates. With our data, we have the prices of different properties in a certain area, we can evaluate if the ratio of rental prices annualized to annual income is above the threshold required for affordability. 

3. As discussed in the introduction, the legality of web scraping is still uncertain in the US. Skim through the Legal Issues section of Web Scraping in the US on Wikipedia and this article about the legal issues with the Computer Fraud and Abuse Act , and describe at least one factor you believe is important to consider when discussing the legality of web scraping and why.

When discussing the legality of web scraping and online data manipulation, the topic of algorithmic transparency is super important to consider because often times webscraping involves discriminatory uses. Certain areas of the web such as flight planning and job search are affected by data algorithms and web scraping but often times these algorithms can be quite predatory and exclude certain groups of people from certain jobs without 100% transparency. Transparency is important in the realm of web scraping legality because it ensure the data will not be used badly or discriminatory. 


4. Scraping public data does not always lead to positive results for society. While web scraping is important for accountability and open access of information, we must also consider issues of privacy as well. Many argue that using someone’s personal data without their consent (even if publicly provided) is unethical. Web scraping requires thoughtful intervention, what are two or more guidelines that must we consider when deciding to use or not to use public data?

One major guideline we must consider when deciding to use public data is the same guideline you use to consider if something is protected under free speech or not. This guideline is whether the data can be used to negatively affect someone. For example, using publicly available birthday info to send ads to young kids negatively affects someone and is harmful. Another guideline to consider is if the benefits for compiling this data outweigh any potential negatives for putting together datasets. For example, compiling data of peoples household income might be useful for generating research on median household incomes of people in specific areas but if scraping this data puts together a list of addresses that can be used in a malicious manner, the negatives outweigh the positives in this instance. There has to be a certain moral line that is kept when dealing with web scraping. 